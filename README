# DIP PROJECT

---

# **Comprehensive Project Report:**
 *Illumination-Invariant Face Recognition*

---

## **Introduction**

This project implements an image processing-based approach for illumination-invariant face recognition. Inspired by the research paper *"A Methodology for Rapid Illumination-Invariant Face Recognition Using Image Processing Filters"* by *Ognjen Arandjelović and Roberto Cipolla*, this project integrates adaptive frameworks, image processing techniques, and similarity metrics to handle face recognition under challenging lighting conditions. The main objective is to create an efficient system that can identify faces even with significant illumination changes while maintaining computational efficiency.

---

## **Project Features**

### Motivation :

In face recognition tasks, illumination conditions and pose play a major bottleneck while trying to get accurate results. We cannot completely rely on image processing pipelines to improve the accuracy under various different conditions. Experiments show that recognition task results on mages in already similar conditions tend to worsen with heavy image processing. We aim to find the balance between image processing techniques and raw images to get the best accuracy in face recognition.

### **Core Functionalities**

1. **Illumination Normalisation**: Utilises self-quotient imaging for robust feature extraction, making the system invariant to variations in lighting conditions.
2. **Similarity Computation**:
    - **Cosine Similarity**: Measures the angular similarity between feature vectors, ideal for comparing image data.
    - **Mutual Subspace Method (MSM)**: Employs subspace comparison for improved robustness, especially in video data where appearance changes due to head motion and pose occur.
3. **Adaptive Density Estimation**:
    - Estimates joint probability density functions that help adjust for illumination differences.
    - Dynamically learns the alpha-function to combine unprocessed and filtered data based on confusion margins. We define the algorithm used for this below.

### **Interactive Command-Line Interface (CLI)**

- **User-Friendly Interface**: Facilitates the selection of query images and gallery paths through command-line arguments.
- **Visualization Options**: Includes tools to display the query image, best matches, and unmatched candidates for visual inspection.

---

## **Project Components**

The project is structured around several key components that align with the methodologies proposed in the research paper *"A Methodology for Rapid Illumination-Invariant Face Recognition Using Image Processing Filters"* by *Arandjelović and Cipolla.* Each component is crucial for handling face recognition under varying illumination conditions effectively.

### **1. Image Preprocessing and Filtering**

- **Self-Quotient Imaging (SQI)**: This technique is used for illumination normalization, a core part of the paper’s methodology. SQI transforms images to enhance their invariance to varying lighting conditions, making them more robust for face recognition.
- **High-Pass Filters (HP)**: Applied to accentuate high-frequency information while reducing low-frequency illumination effects. This aligns with the filtering methods discussed in the paper for better illumination normalization.
- **Additional Filters**: Filters like the Laplacian-of-Gaussian (LG) and edge detection (Canny and directional derivatives) were explored for their efficacy in enhancing discriminative facial features.

### **2. Similarity Metrics and Face Matching Techniques**

- **Cosine Similarity**: A fundamental metric used to compute the angular similarity between feature vectors of the query and gallery images. This is a basic approach that was employed to establish a baseline for recognition.
- **Mutual Subspace Method (MSM)**: This method was included for its ability to compare subspaces formed by video sequences, crucial for handling the variation in pose and head motion as discussed in the paper. MSM is noted for its robustness and stability in recognizing faces under complex conditions.

### **3. Adaptive Framework for Illumination Handling**

The algorithm used for offline training is given as follows : 

Input:

- `training data D(person, illumination)`
- `filtered data F(person, illumination)`
- `similarity function p`

Output:

- `estimate p(α, µ)`
1. `Initialization
p̂(α, µ) = 0`
2. `Simulated matching iteration
for all illuminations i, j and persons p`
    1. `Confusion margin
    µ = ρ(D(p, i), D(r1, j)) - ρ(D(p, i), D(r2, j))`
    2. `Iteration
    for all k = 0, ..., 1/Δα, α = kΔα`
        1. `Quantify performance of α
        δ(kΔα) = αρ(D(p,i),D(p,j))+(1-α)ρ(F(p,i),F(p,j))
        --------------------------------------------------
        max{p≠q}[αρ(D(p,i),D(q,j))+(1-α)ρ(F(p,i),F(q,j))]`
        2. `Update density estimate
        p̂(kΔα, µ) = p̂(kΔα, µ) + δ(kΔα)`
3. `Smooth the output
p̂(α, µ) = p̂(α, µ) * Gσ=0.05`
4. `Normalize to unit integral
p̂(α, µ) = p̂(α, µ) / ∫∫ p̂(α, µ) dµ dα`

- **Joint Probability Density Estimation**: A key aspect of the paper's approach is the estimation of joint probability density functions to understand the relationship between the query and gallery images under different lighting. This density estimation helps in identifying the optimal alpha value.
- **Alpha-Function Estimation**: The adaptive system learns the alpha-function, **`α*`**, which represents the optimal weighting of raw and filtered data based on the "confusion margin" (the difference between the top two matching scores). This dynamic adjustment is critical for improving recognition accuracy in diverse illumination scenarios.

### **4. Online Face Matching Workflow**

- **Integration of Filters and Similarity Methods**: The project integrates raw image data with filtered representations, applying the learned alpha-function to balance these contributions effectively. This combination allows for more accurate matching, especially in varying illumination conditions.
- **Decision Fusion Strategy**: The paper's methodology suggests combining unprocessed and filtered image data to yield a single matching score. The fusion strategy applies the learned alpha-function to adjust the contributions based on the confusion margin, ensuring optimal matching performance.

### **5. Interactive Command-Line Interface (CLI)**

- **User Input Handling**: The CLI enables users to input paths for the query image and gallery, select similarity methods, and initiate the face-matching process.
- **Visualization Tools**: Implements interactive displays using **`matplotlib`** to preview the query image, best match, and unmatched results, aiding in user verification and analysis.

### **6. Empirical Evaluation and Results Analysis**

- **Dataset Utilization**: The project uses datasets like **YaleDB** to validate the performance of the system, as illustrated in the research paper's experiments.
- **Performance Metrics**: The system evaluates recognition accuracy, error rate reduction, and variability in performance across different filtering techniques and similarity methods. Results are compared against baseline methods to demonstrate improvements in recognition rates and stability under varying lighting conditions.

Each of these components is designed to align with the insights and techniques proposed in the paper, contributing to an effective illumination-invariant face recognition system

## **Core Functions**

- **`load_image()`**: Loads images from specified paths, facilitating data handling and preprocessing.
- **`preview_image()`**: Displays images for visualization using `matplotlib`.
- **`self_quotient_image()`**: Performs illumination normalization to improve face recognition.
- **`cosine_similarity()`**: Computes similarity scores between vectors.
- **`mutual_subspace_method()`**: A sophisticated method for comparing feature sub-spaces in video sequences.
- **`plot_density()`**: Visualises the density landscape for alpha-function estimation.

---

### **How to Use**

1. **Install Requirements**
    
    Install dependencies using:
    
    ```bash
    pip install -r requirements.txt
    ```
    
2. **Prepare Data**
    - Place **`.jpg`** gallery images in a directory. (Labelled as per required naming system)
    - Have a query **`.jpg`** image ready.
3. **Run the App**
    
    Execute the script in the `cli`  folder:
    
    ```bash
    python driver.py
    ```
    
4. **Follow Prompts**
    - Input gallery and query paths.
    - Select similarity and filter methods.
    - Configure optional settings like ASCII art preview.
5. **Menu Options**
    - Preview images.
    - View best match.
    - Use a pretrained model.
    - Visualize similarities.
    - Exit when done.
6. **Results**
    
    Outputs include similarity scores, best match details, and optional visualizations saved in **`output/`**
    

---

## **Results**

### **Evaluation Setup**

The algorithm was evaluated using a variety of datasets to assess its robustness under different conditions:

- **YaleDB**: Single image per subject, varied illumination conditions and under various expressions.

### **Key Metrics**

- **Recognition Accuracy**:
    - **Cosine Similarity**:  Cosine similarity measures the similarity between two images by treating them as high-dimensional vectors and comparing their orientations in the feature space. It is defined as the cosine of the angle between two vectors, which corresponds to how aligned the images are in the feature space.
    - **MSM with Self-Quotient Imaging**: The **Mutual Subspace Method (MSM)** is a technique commonly used for recognizing patterns or objects, especially in computer vision tasks like image and video recognition. MSM measures the similarity between two sets of high-dimensional data by comparing the subspaces they span rather than individual data points.
- **Error Rate Reduction**:
    - Improvement in unfiltered and processed scores of the images give an idea of how the metric is improving the accuracy of the predictions.

### Image Processing Pipelines

Filters used are as follows, motivated by their capability to retain information about lighting and shadows in the image

![image.png](./REPORT/image.png)

These filters store data about the image which doesn’t easily get captured in raw image especially under different lighting.

### **Graphical Results**

1. **Error Rate Comparison**:
    - Average recognition improvement rates across different methods and filters.
    
    For example : this is for the self-quotient image with the cosine similarity metric.
    
    ![image.png](./REPORT/image%201.png)
    
    - This shows that for images less similar to the query image, the similarity difference between the adaptive similarity and normal similarity scales indicating that it performs better to get less similar images to better recognition.
2. **Density Landscape**:
    - Density estimation of the alpha-function, showing optimal weights for combining raw and filtered data.
    - The below landscape depicts probability of a particular weight **`α`** for a given confusion margin where confusion margin is the difference in unprocessed similarities between top 2 similar images
    
    ![image.png](./REPORT/image%202.png)
    
    ![image.png](./REPORT/image%203.png)
    
3. **Performance :**

We can see that for a query image a trained model gives better recognition rates and the top similarities achieved are accurate. For a quotient image pipeline with cosine similarity, we get the following results:

![image.png](./REPORT/image%204.png)

Here, we see that the combined similarity (which uses adaptive weights by estimating optimal alpha) is more than what raw images can achieve by themselves.

1. **Why not image processing alone:** 

![image.png](./REPORT/image%205.png)

---

For filtered images, we lose information and it rates all images similar in similar conditions thus masking the actual similarities associated with the illumination conditions. Thus we cannot heavily rely on image processing pipelines to get interpretable results. We can say that the adaptive algorithm can **smoothen the output**.

## **Learning Overview**

### **Theoretical Underpinnings**

- **Illumination Normalization**: Self-quotient imaging was applied to transform images to a more invariant form under varying lighting.
- **Similarity Computation**: The cosine similarity metric measured basic vector similarity, while MSM was used for subspace comparison, optimizing recognition in video-based datasets.
- **Adaptive Framework**: Used joint probability density to learn the optimal alpha-value that balances raw and filtered data contributions.

### **Practical Implementations**

- **Offline Training**: The algorithm learns the alpha-function `α*` offline, leveraging data distributions for better online performance.
- **Adaptive Fusion**: The alpha-function was trained to adjust the weighting between raw and filtered inputs based on the confusion margin `l`.

### **Lessons Learned**

- **Illumination Invariance**: Proper filtering techniques can significantly enhance face recognition under non-uniform lighting conditions.
- **Adaptive Learning**: Using data-driven estimations for optimal alpha values provides better generalization compared to fixed parameters.
- **Complexity vs. Performance Trade-off**: While more complex filters like the edge map sometimes performed poorly, simpler filters like high-pass and quotient images showed consistent improvements.

---

## Challenges

- **Skewed training data**: Most of the images we had access to didn’t perform well on traditional face recognition tasks with the samples of the same individual making adaptive weighting still inefficient since the magnitude of inaccuracy was too high to recover.
- Here is an example query which doesn’t do well on both processed and unprocessed data.

![image.png](./REPORT/image%206.png)

- **Lack of enough examples:** In the dataset we used, we had only 15 subjects under just three illumination conditions.  This makes it hard to learn enough parameters to estimate illumination invariance pipeline completely.
- **Extreme illuminations**: Extreme illumination condition differences say a left illumination and an extreme right illumination were hard to adapt to even with the adaptive model.
- **Lack of compute powe**r: Training for even a 100 epoch on a 100*100 grid takes 1 hour, so this took a lot of patience even for extremely simple runs.

## **Future Enhancements**

1. **Integration with Deep Learning**: Replace handcrafted filters with deep learning models for more robust feature extraction.
2. **Web Interface**: Develop an intuitive web-based user interface for more accessible use.
3. **Real-Time Performance**: Optimize for GPU acceleration to achieve near real-time processing capabilities.
4. **Improved Filters**: Investigate advanced filters capable of handling extreme cases like cast shadows and specular reflections.

---

## **Conclusion**

This project effectively demonstrated an illumination-invariant face recognition system that performed well across challenging datasets. By combining adaptive frameworks, advanced similarity metrics, and image normalization techniques, it achieved significant performance improvements. This paves the way for future research and real-world applications in dynamic and uncontrolled environments.

**References**

- *Arandjelović, O., Cipolla, R.* "A Methodology for Rapid Illumination-Invariant Face Recognition Using Image Processing Filters”
- Relevant literature on cosine similarity, MSM, and illumination normalization techniques

---

*Link To Implementation Repo:*  [divyanash911/Illumination-invariance-face-recognition at master](https://github.com/divyanash911/Illumination-invariance-face-recognition/tree/master)

---